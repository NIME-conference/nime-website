<!-- copied out of alembic's default template. -->
<!doctype html>
<html lang="en-GB">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="manifest" href="/nime/manifest.json">
    <meta name="theme-color" content="#242e2b"/>
		<link rel="stylesheet" href="/nime/assets/styles.css">
    
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@16px.png" sizes="16x16">
    <link rel="apple-touch-icon" sizes="16x16" href="/assets/logos/logo@16px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@32px.png" sizes="32x32">
    <link rel="apple-touch-icon" sizes="32x32" href="/assets/logos/logo@32px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@96px.png" sizes="96x96">
    <link rel="apple-touch-icon" sizes="96x96" href="/assets/logos/logo@96px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@120px.png" sizes="120x120">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/logos/logo@120px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@144px.png" sizes="144x144">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/logos/logo@144px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@180px.png" sizes="180x180">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/logos/logo@180px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@512px.png" sizes="512x512">
    <link rel="apple-touch-icon" sizes="512x512" href="/assets/logos/logo@512px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@1024px.png" sizes="1024x1024">
    <link rel="apple-touch-icon" sizes="1024x1024" href="/assets/logos/logo@1024px.png">
  

<link rel="shortcut icon" href="/assets/logos/nime.png">

		<!-- Overwrite this file with code you want before the closing head tag -->


		
  </head>

	<body>
		<header class="header">
  <div class="container">
    <a class="logo" href="/nime/">
  <img src="/nime/assets/logos/nime.png" alt="NIME logo">
</a>


    
<nav class="nav  nav--header">
  <ul class="list  list--nav">
    

      

      <li class="item  item--nav">
        <a href="/nime/">Home</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/past-nimes/">Past NIMEs</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/archives/">Proceedings</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/music/">Music</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/hosting/">Hosting</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/committee/">Committee</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/list/">Mailing List</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/statements/">Statements</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="https://forum.nime.org/">Forum</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/nime/mentorship/">Mentorship</a>
      </li>
    
  </ul>
  <button class="button  button--nav" aria-label="Menu toggle">
    <svg width="16" height="16" class="icon  icon--nav" role="img" alt="Menu"><title>Menu</title>
<use xlink:href="#nav" fill="CurrentColor"></use></svg>

  </button>
</nav>


<script type="text/javascript">
  // Get list and button
  const navList = document.querySelector('.header .list--nav')
  const button  = document.querySelector('.header .button--nav')

  // Hide nav and apply toggle
  const collapseNav = () => {
    if (document.body.clientWidth < 640) {
      navList.style.setProperty('--listHeight', `-${navList.offsetHeight}px`)
    } else {
      navList.removeAttribute('style')
    }

    button.onclick = () => {
      navList.style.setProperty('transition', `margin .1s`)
      if (navList.style.getPropertyValue('--listHeight')) {
        navList.style.removeProperty('--listHeight')
      } else {
        navList.style.setProperty('--listHeight', `-${navList.offsetHeight}px`)
      }
    }
  }

  collapseNav()

  // Check on resize if to collapse nav
  window.addEventListener('resize', () => {
    collapseNav()
  })
</script>

  </div>

  






</header>

		<main class="main container">
			<article class="article article--page content typeset">
				
				<h1>Generating Convincing Harmony Parts with Simple Long Short-Term Memory Networks</h1>
				
				<h4>



Andrei Faitas, Synne Engdahl Baumann, Torgrim Rudland Næss, Jim Torresen, and Charles Patrick Martin</h4>
				
				<p>Proceedings of the International Conference on New Interfaces for Musical Expression</p>
				
				<ul>
					<li>Year: 2019</li>
					<li>Location: Porto Alegre, Brazil</li>
					
					<li>Pages: 325–330</li>
					
					
					<li>DOI: <a href="https://doi.org/10.5281/zenodo.3672980">10.5281/zenodo.3672980</a>
</li>
					<li><a href="http://www.nime.org/proceedings/2019/nime2019_paper062.pdf">PDF link</a></li>
					
					
					
					
				</ul>
				
				<h2>Abstract:</h2>
				
				<p>Generating convincing music via deep neural networks is a challenging problem that shows promise for many applications including interactive musical creation. One part of this challenge is the problem of generating convincing accompaniment parts to a given melody, as could be used in an automatic accompaniment system. Despite much progress in this area, systems that can automatically learn to generate interesting sounding, as well as harmonically plausible, accompanying melodies remain somewhat elusive. In this paper we explore the problem of sequence to sequence music generation where a human user provides a sequence of notes, and a neural network model responds with a harmonically suitable sequence of equal length. We consider two sequence-to-sequence models; one featuring standard unidirectional long short-term memory (LSTM) architecture, and the other featuring bidirectional LSTM, both successfully trained to produce a sequence based on the given input. Both of these are fairly dated models, as part of the investigation is to see what can be achieved with such models. These are evaluated and compared via a qualitative study that features 106 respondents listening to eight random samples from our set of generated music, as well as two human samples. From the results we see a preference for the sequences generated by the bidirectional model as well as an indication that these sequences sound more human.</p>

				<h2>Citation:</h2>

				



Andrei Faitas, Synne Engdahl Baumann, Torgrim Rudland Næss, Jim Torresen, and Charles Patrick Martin. 2019. 


Generating Convincing Harmony Parts with Simple Long Short-Term Memory Networks. 


<em>Proceedings of the International Conference on New Interfaces for Musical Expression</em>. 
DOI: <a href="https://doi.org/10.5281/zenodo.3672980">10.5281/zenodo.3672980</a>



				<h2>BibTeX Entry:</h2>

<pre>
@inproceedings{Faitas2019,
 abstract = {Generating convincing music via deep neural networks is a challenging problem that shows promise for many applications including interactive musical creation. One part of this challenge is the problem of generating convincing accompaniment parts to a given melody, as could be used in an automatic accompaniment system. Despite much progress in this area, systems that can automatically learn to generate interesting sounding, as well as harmonically plausible, accompanying melodies remain somewhat elusive. In this paper we explore the problem of sequence to sequence music generation where a human user provides a sequence of notes, and a neural network model responds with a harmonically suitable sequence of equal length. We consider two sequence-to-sequence models; one featuring standard unidirectional long short-term memory (LSTM) architecture, and the other featuring bidirectional LSTM, both successfully trained to produce a sequence based on the given input. Both of these are fairly dated models, as part of the investigation is to see what can be achieved with such models. These are evaluated and compared via a qualitative study that features 106 respondents listening to eight random samples from our set of generated music, as well as two human samples. From the results we see a preference for the sequences generated by the bidirectional model as well as an indication that these sequences sound more human.},
 address = {Porto Alegre, Brazil},
 author = {Andrei Faitas and Synne Engdahl Baumann and Torgrim Rudland Næss and Jim Torresen and Charles Patrick Martin},
 booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
 doi = {10.5281/zenodo.3672980},
 editor = {Marcelo Queiroz and Anna Xambó Sedó},
 issn = {2220-4806},
 month = {June},
 pages = {325--330},
 publisher = {UFRGS},
 title = {Generating Convincing Harmony Parts with Simple Long Short-Term Memory Networks},
 url = {http://www.nime.org/proceedings/2019/nime2019_paper062.pdf},
 year = {2019}
}

</pre>
			</article>
		</main>
		<footer class="footer">
  <div class="container">
    <div class="copyright  typeset">
      <small class="small">© NIME 2024</small>
    </div>

    
<nav class="nav  nav--footer">
  <ul class="list list--nav">
    

      

      <li class="item  item--nav">
        <a href="/nime/#top">Back to top</a>
      </li>
    
  </ul>
</nav>


  </div>
</footer>

	</body>
</html>
